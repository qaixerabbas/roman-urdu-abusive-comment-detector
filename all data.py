# -*- coding: utf-8 -*-
"""deep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10u3uhh9MmTw86fpR068xExkyWB9z0rlM
"""

import pandas as pd
data = pd.read_csv('dataset.csv')

import tensorflow as tf
tf.test.gpu_device_name()

def getVectors(corpus,vectors,size):
    wordset = set(vectors.wv.index2word) #Checks if the word is in the Word2vec corpus 
    vec = []
    counter = 0
    for words in corpus:    
        featureVec = np.zeros(size,dtype="object")
        for word in words:
            if word in wordset:
                featureVec = np.add(featureVec,vectors[word])
        vec.append(featureVec.T)
        counter = counter + 1
        #print(counter)
    return vec

corpus = []
for words in data.comment:
    words = words.split()
    corpus.append(words)

X = corpus

from keras.utils import to_categorical
Y = to_categorical(data.rating)

import gensim.models.word2vec as wv
vocab_size = 300
min_counts = 10
context = 5
n_workers = 15
down_sample = 1e-2
vectors = wv.Word2Vec(X,
                     size = vocab_size,
                     workers =n_workers,
                     window = context,
                     min_count = min_counts,
                     sample = down_sample)

import numpy as np

X_train = getVectors(X,vectors,vocab_size)

X_train = np.array(X_train)

from keras.models import Sequential
from keras.layers import Dense
from keras import layers
from keras import regularizers

classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(output_dim =1024, init = 'uniform', activation = 'relu', input_dim = vocab_size,kernel_regularizer= regularizers.l2(0.0001)))
classifier.add(layers.Dropout(0.2))
# Adding the second hidden layer
classifier.add(Dense(output_dim =1024, init = 'uniform', activation = 'relu'))
#classifier.add(layers.Dropout(0.25))
# Adding the third hidden layer
classifier.add(Dense(output_dim =1024, init = 'uniform', activation = 'relu',kernel_regularizer= regularizers.l2(0.0001)))
#classifier.add(layers.Dropout(0.25))
# Adding the output layer
classifier.add(Dense(output_dim = 2, init = 'uniform', activation = 'softmax'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
classifier.fit(X_train, Y, batch_size = 50, epochs = 60)

classifier.save("deep.h5")

import pickle as p
f = open("wv.pickle","wb")
p.dump(vectors,f)
f.close()